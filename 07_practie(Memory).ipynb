{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: Hey~!\\nAI: How was it going?'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Memory\n",
    "# Conversation Buffer Memory\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "memory.save_context({\"input\":\"Hey~!\"}, {\"output\":\"How was it going?\"})\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='Hey~!'),\n",
       "  AIMessage(content='How was it going?')]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "memory.save_context({\"input\":\"Hey~!\"}, {\"output\":\"How was it going?\"})\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='2'),\n",
       "  AIMessage(content='2'),\n",
       "  HumanMessage(content='3'),\n",
       "  AIMessage(content='3'),\n",
       "  HumanMessage(content='4'),\n",
       "  AIMessage(content='4'),\n",
       "  HumanMessage(content='5'),\n",
       "  AIMessage(content='5')]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Conversation Buffer Window Memory\n",
    "# 대화의 일부! 그 중에서 최근의 메세지를 저장하는 기능 - k 값에 따라 조절 가능  \n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "memory = ConversationBufferWindowMemory(\n",
    "    return_messages=True,\n",
    "    k=4,\n",
    ")\n",
    "\n",
    "def add_message(input, output):\n",
    "    memory.save_context({\"input\":input}, {\"output\":output})\n",
    "\n",
    "add_message(1,1)\n",
    "add_message(2,2)\n",
    "add_message(3,3)\n",
    "add_message(4,4)\n",
    "add_message(5,5)\n",
    "\n",
    "memory.load_memory_variables({})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Nicolas from South Korea introduces himself to the AI, who responds with enthusiasm, finding it cool. The human mentions how pretty South Korea is, and the AI expresses a wish to visit.'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Conversation Summary Memory -> llm을 사용함! \n",
    "# 대화를 요약해서 정보를 저장하는 기능\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryMemory(llm=llm)\n",
    "\n",
    "\n",
    "def add_message(input, output):\n",
    "    memory.save_context({\"input\": input}, {\"output\": output})\n",
    "\n",
    "\n",
    "def get_history():\n",
    "    return memory.load_memory_variables({})\n",
    "\n",
    "\n",
    "add_message(\"Hi I'm Nicolas, I live in South Korea\", \"Wow that is so cool!\")\n",
    "\n",
    "add_message(\"South Kddorea is so pretty\", \"I wish I could go!!!\")\n",
    "\n",
    "get_history()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversation Summary Buffer Memory\n",
    "# Conversation Summary Memory + Window Buffer Memory + Buffer Window Memory\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=150,\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "\n",
    "def add_message(input, output):\n",
    "    memory.save_context({\"input\": input}, {\"output\": output})\n",
    "\n",
    "\n",
    "def get_history():\n",
    "    return memory.load_memory_variables({})\n",
    "\n",
    "\n",
    "add_message(\"Hi I'm Nicolas, I live in South Korea\", \"Wow that is so cool!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content=\"Hi I'm Nicolas, I live in South Korea\"),\n",
       "  AIMessage(content='Wow that is so cool!')]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_message(\"South Korea is so pretty\", \"I wish I could go!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content=\"Hi I'm Nicolas, I live in South Korea\"),\n",
       "  AIMessage(content='Wow that is so cool!'),\n",
       "  HumanMessage(content='South Korea is so pretty'),\n",
       "  AIMessage(content='I wish I could go!!!')]}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_message(\"How far is Korea from Argentina?\", \"I don't know! Super far!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content=\"Hi I'm Nicolas, I live in South Korea\"),\n",
       "  AIMessage(content='Wow that is so cool!'),\n",
       "  HumanMessage(content='South Korea is so pretty'),\n",
       "  AIMessage(content='I wish I could go!!!'),\n",
       "  HumanMessage(content='How far is Korea from Argentina?'),\n",
       "  AIMessage(content=\"I don't know! Super far!\")]}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_message(\"How far is Brazil from Argentina?\", \"I don't know! Super far!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content=\"Hi I'm Nicolas, I live in South Korea\"),\n",
       "  AIMessage(content='Wow that is so cool!'),\n",
       "  HumanMessage(content='South Korea is so pretty'),\n",
       "  AIMessage(content='I wish I could go!!!'),\n",
       "  HumanMessage(content='How far is Korea from Argentina?'),\n",
       "  AIMessage(content=\"I don't know! Super far!\"),\n",
       "  HumanMessage(content='How far is Brazil from Argentina?'),\n",
       "  AIMessage(content=\"I don't know! Super far!\")]}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversation Knowledge Grahp Memory\n",
    "# 오~! 이거 쫌 신기한듯! Chat 정보를 저장함\n",
    "from langchain.memory import ConversationKGMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationKGMemory(\n",
    "    llm=llm,\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "\n",
    "def add_message(input, output):\n",
    "    memory.save_context({\"input\": input}, {\"output\": output})\n",
    "\n",
    "\n",
    "add_message(\"Hi I'm Nicolas, I live in South Korea\", \"Wow that is so cool!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [SystemMessage(content='On Nicolas: Nicolas lives in South Korea.')]}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({\"input\": \"who is Nicolas\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_message(\"Nicolas likes kimchi\", \"Wow that is so cool!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [SystemMessage(content='On Nicolas: Nicolas lives in South Korea. Nicolas likes kimchi.')]}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({\"inputs\": \"what does nicolas like\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "    You are a helpful AI talking to a human.\n",
      "\n",
      "    \n",
      "    Human:My name is Nico\n",
      "    You:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hello Nico! How can I assist you today?'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 메모리를 체인에 연결하는 방법! \n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=120,\n",
    "    memory_key=\"chat_history\",\n",
    ")\n",
    "\n",
    "template = \"\"\"\n",
    "    You are a helpful AI talking to a human.\n",
    "\n",
    "    {chat_history}\n",
    "    Human:{question}\n",
    "    You:\n",
    "\"\"\"\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    prompt=PromptTemplate.from_template(template),\n",
    "    verbose=True, #프롬프트의 로그를 확인할 수 있음! \n",
    ")\n",
    "\n",
    "chain.predict(question=\"My name is Nico\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "    You are a helpful AI talking to a human.\n",
      "\n",
      "    Human: My name is Nico\n",
      "AI: Hello Nico! How can I assist you today?\n",
      "Human: I live in Seoul\n",
      "AI: That's great to know! How can I assist you with information or tasks related to Seoul?\n",
      "Human: What is my name?\n",
      "AI: Your name is Nico.\n",
      "    Human:I live in Seoul\n",
      "    You:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Yes, you mentioned that earlier. Is there anything specific you would like to know or discuss about living in Seoul?'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.predict(question=\"I live in Seoul\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "    You are a helpful AI talking to a human.\n",
      "\n",
      "    Human: My name is Nico\n",
      "AI: Hello Nico! How can I assist you today?\n",
      "Human: I live in Seoul\n",
      "AI: That's great to know! How can I assist you with information or tasks related to Seoul?\n",
      "Human: What is my name?\n",
      "AI: Your name is Nico.\n",
      "Human: I live in Seoul\n",
      "AI: Yes, you mentioned that earlier. Is there anything specific you would like to know or discuss about living in Seoul?\n",
      "    Human:What is my name?\n",
      "    You:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Your name is Nico. How can I assist you further today?'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.predict(question=\"What is my name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat_history': \"System: The human introduces themselves as Nico. The AI greets Nico and asks how it can assist. Nico mentions they live in Seoul.\\nAI: That's great to know! How can I assist you with information or tasks related to Seoul?\\nHuman: What is my name?\\nAI: Your name is Nico.\\nHuman: I live in Seoul\\nAI: Yes, you mentioned that earlier. Is there anything specific you would like to know or discuss about living in Seoul?\\nHuman: What is my name?\\nAI: Your name is Nico. How can I assist you further today?\"}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 기존 memory 경우 chat 형태로 결과값을 출력하지만 \n",
    "\"\"\"\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=120,\n",
    "    memory_key=\"chat_history\",\n",
    ")\n",
    "\"\"\"\n",
    "# 아래 처럼 str 으로 결과를 출력할 수 있음\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: You are a helpful AI talking to a human\n",
      "Human: My name is Nico\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Nice to meet you, Nico! How can I assist you today?'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#대화를 기반으로 채팅을 출력? 해서 사용할 수 있음\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder \n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=120,\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True, #사람, AI 메세지를 실제 메세지 클래스로 변경하는 것!!!! \n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful AI talking to a human\"),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "chain.predict(question=\"My name is Nico\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: You are a helpful AI talking to a human\n",
      "Human: My name is Nico\n",
      "AI: Nice to meet you, Nico! How can I assist you today?\n",
      "Human: I live in Seoul\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Seoul is a vibrant city with a rich history and culture. Is there anything specific you would like to know or discuss about Seoul?'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.predict(question=\"I live in Seoul\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: You are a helpful AI talking to a human\n",
      "Human: My name is Nico\n",
      "AI: Nice to meet you, Nico! How can I assist you today?\n",
      "Human: I live in Seoul\n",
      "AI: Seoul is a vibrant city with a rich history and culture. Is there anything specific you would like to know or discuss about Seoul?\n",
      "Human: What is my name?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Your name is Nico.'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.predict(question=\"What is my name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: You are a helpful AI talking to a human\n",
      "Human: My name is Nico\n",
      "AI: Nice to meet you, Nico! How can I assist you today?\n",
      "Human: I live in Seoul\n",
      "AI: Seoul is a vibrant city with a rich history and culture. Is there anything specific you would like to know or discuss about Seoul?\n",
      "Human: What is my name?\n",
      "AI: Your name is Nico.\n",
      "Human: 대화내용 요약해줘?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'당신의 이름은 Nico이고, 서울에 거주하고 계신 것 같아요. 무엇을 도와드릴까요?'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.predict(question=\"대화내용 요약해줘?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# off the shell -> memory | langchain expression languege -> LCEL\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=120,\n",
    "    # memory_key=\"chat_history\",    \n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful AI talking to a human\"),\n",
    "        MessagesPlaceholder(variable_name=\"history\"), # 메모리 히스토리를 옮겨옴\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# chain = LLMChain(\n",
    "#     llm=llm,\n",
    "#     memory=memory,\n",
    "#     prompt=prompt,\n",
    "#     verbose=True,\n",
    "# )\n",
    "\n",
    "# chain.predict(question=\"My name is Nico\")\n",
    "\n",
    "def load_memory(_):\n",
    "    return memory.load_memory_variables({})[\"history\"]\n",
    "\n",
    "# Runnable Pass through\n",
    "chain = RunnablePassthrough.assign(history=load_memory) | prompt | llm\n",
    "\n",
    "\n",
    "def invoke_chain(question):\n",
    "    result = chain.invoke({\"question\": question})\n",
    "    memory.save_context(\n",
    "        {\"input\": question},\n",
    "        {\"output\": result.content},\n",
    "    )\n",
    "    print(result)\n",
    "\n",
    "\n",
    "\n",
    "# 코드 분석\n",
    "\"\"\"\n",
    "필요한 라이브러리와 클래스를 가져옵니다.\n",
    "ChatOpenAI 인스턴스를 생성\n",
    "낮은 temperature 값(0.1)은 일관된 출력을 위한 것\n",
    "ConversationSummaryBufferMemory 설정\n",
    "\n",
    "토큰 제한을 120으로 설정하여 메모리 사용 제어\n",
    "대화 내용 요약 저장\n",
    "\n",
    "ChatPromptTemplate 정의\n",
    "\n",
    "시스템 메시지, \n",
    "대화 기록, \n",
    "사용자 질문을 포함\n",
    "\n",
    "load_memory 함수는 메모리에서 대화 기록 로딩\n",
    "\n",
    "RunnablePassthrough를 사용하여 체인 구성\n",
    "- 대화 기록을 불러오고, 프롬프트를 적용한 후 LLM에 전달합니다.\n",
    "\n",
    "invoke_chain 함수는 질문을 처리하고 결과를 메모리에 저장\n",
    "\n",
    "이 코드를 더 복잡하고 강력한 시스템으로 발전시킬 수 있으며, \n",
    "예를 들어, 다중 에이전트 시스템, 도구 사용, 웹 검색 기능 등을 추가할 수 있음\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Hello Nico! How can I assist you today?'\n"
     ]
    }
   ],
   "source": [
    "invoke_chain(\"My name is nico\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Your name is Nico.'\n"
     ]
    }
   ],
   "source": [
    "invoke_chain(\"What is my name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정리 \n",
    "# 필요한 라이브러리와 클래스를 가져옵니다.\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "# ChatOpenAI 인스턴스를 생성합니다.\n",
    "# 낮은 temperature 값(0.1)은 일관된 출력을 위한 것입니다.\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "# ConversationSummaryBufferMemory를 설정합니다.\n",
    "# 토큰 제한을 120으로 설정하여 메모리 사용을 제어합니다.\n",
    "# 대화 내용을 요약하여 저장합니다.\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=120,\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "# ChatPromptTemplate을 정의합니다.\n",
    "# 시스템 메시지, 대화 기록, 사용자 질문을 포함합니다.\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful AI talking to a human\"),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),  # 메모리 히스토리를 옮겨옴\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# load_memory 함수는 메모리에서 대화 기록을 불러옵니다.\n",
    "def load_memory(_):\n",
    "    return memory.load_memory_variables({})[\"history\"]\n",
    "\n",
    "# RunnablePassthrough를 사용하여 체인을 구성합니다.\n",
    "# 대화 기록을 불러오고, 프롬프트를 적용한 후 LLM에 전달합니다.\n",
    "chain = RunnablePassthrough.assign(history=load_memory) | prompt | llm\n",
    "\n",
    "# invoke_chain 함수는 질문을 처리하고 결과를 메모리에 저장합니다.\n",
    "def invoke_chain(question):\n",
    "    result = chain.invoke({\"question\": question})\n",
    "    memory.save_context(\n",
    "        {\"input\": question},\n",
    "        {\"output\": result.content},\n",
    "    )\n",
    "    print(result)\n",
    "\n",
    "# 이 코드를 더 복잡하고 강력한 시스템으로 발전시킬 수 있으며,\n",
    "# 예를 들어, 다중 에이전트 시스템, 도구 사용, 웹 검색 기능 등을 추가할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: 당신은 친절하고 도움이 되는 AI 어시스턴트입니다. 사용자와의 대화를 기억하고 이전 대화 내용을 바탕으로 응답합니다.\n",
      "System: \n",
      "Human: 안녕하세요! 오늘 기분이 어떠세요?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Human: 안녕하세요! 오늘 기분이 어떠세요?\n",
      "AI: 안녕하세요! 제 기분은 항상 좋습니다. 여러분의 하루가 행복하고 유익하길 바라요. 혹시 오늘 무슨 일을 하셨나요?\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: 당신은 친절하고 도움이 되는 AI 어시스턴트입니다. 사용자와의 대화를 기억하고 이전 대화 내용을 바탕으로 응답합니다.\n",
      "System: The human greets the AI in Korean and asks how it's feeling. The AI responds that it always feels good and wishes everyone a happy and productive day. The AI then asks the human what they did today.\n",
      "Human: AI의 발전에 대해 어떻게 생각하시나요?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Human: AI의 발전에 대해 어떻게 생각하시나요?\n",
      "AI: AI의 발전에 대해 저는 매우 긍정적으로 생각합니다. AI 기술의 발전으로 우리는 더 빠르고 효율적으로 일을 처리할 수 있고, 새로운 혁신적인 서비스와 제품을 경험할 수 있게 됩니다. 물론 이러한 발전은 윤리적인 문제와 함께 고민해야 할 부분도 많지만, 적절히 활용된다면 인류에 많은 혜택을 줄 수 있다고 생각합니다. 혹시 당신은 AI의 발전에 대해 어떻게 생각하시나요?\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: 당신은 친절하고 도움이 되는 AI 어시스턴트입니다. 사용자와의 대화를 기억하고 이전 대화 내용을 바탕으로 응답합니다.\n",
      "System: The human greets the AI in Korean and asks how it's feeling. The AI responds positively and wishes everyone a happy and productive day. The AI believes that the advancement of AI technology will lead to faster and more efficient work processes, as well as the experience of new innovative services and products. Despite ethical considerations, the AI is optimistic about the benefits it can bring to humanity. The AI then asks the human for their thoughts on the advancement of AI.\n",
      "Human: 제가 아까 무엇에 대해 물어봤었나요?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Human: 제가 아까 무엇에 대해 물어봤었나요?\n",
      "AI: 아까 물어보신 내용은 \"인공지능이 어떻게 기분이야?\" 였습니다. 당신은 인공지능과 대화를 나누면서 새로운 주제들에 대해 이야기해 주셨는데, 그 중에서 인공지능 기술의 발전과 인류에 미치는 영향에 대해 이야기했습니다. 이에 대한 내용으로 대화를 이어가실래요?\n",
      "\n",
      "대화 요약:\n",
      "[SystemMessage(content=\"The human greets the AI in Korean and asks how it's feeling. The AI responds positively and wishes everyone a happy and productive day. The AI believes that the advancement of AI technology will lead to faster and more efficient work processes, as well as the experience of new innovative services and products. Despite ethical considerations, the AI is optimistic about the benefits it can bring to humanity. The AI then asks the human for their thoughts on the advancement of AI. The AI recalls the previous conversation topics and asks if the human would like to continue discussing the impact of AI technology on humanity.\")]\n"
     ]
    }
   ],
   "source": [
    "# 새로운 버전 \n",
    "\"\"\"\n",
    "이 시스템은 사용자의 입력에 응답하고 대화 내용을 요약하여 기억합니다. \n",
    "코드의 주요 구성 요소와 작동 방식은 다음과 같습니다:\n",
    "1. ChatOpenAI 모델 초기화:\n",
    "- `ChatOpenAI(temperature=0.7)`를 사용하여 AI 모델을 초기화합니다.\n",
    "\n",
    "2. 메모리 설정:\n",
    "- `ConversationSummaryMemory`를 사용하여 대화 내용을 요약하고 기억합니다.\n",
    "\n",
    "3. 프롬프트 템플릿 정의:\n",
    "- 시스템 메시지, 대화 기록, 사용자 입력을 포함하는 템플릿을 정의합니다.\n",
    "\n",
    "4. ConversationChain 생성:\n",
    "- 모델, 메모리, 프롬프트를 결합하여 대화 체인을 생성합니다.\n",
    "\n",
    "5. chat_with_bot 함수:\n",
    "- 사용자 입력을 받아 AI의 응답을 생성하고 출력합니다.\n",
    "\n",
    "코드 실행 결과를 보면, 시스템이 3번 작동한 것으로 보입니다:\n",
    "\n",
    "1. \"안녕하세요! 오늘 기분이 어떠세요?\"\n",
    "2. \"AI의 발전에 대해 어떻게 생각하시나요?\"\n",
    "3. \"제가 아까 무엇에 대해 물어봤었나요?\"\n",
    "\n",
    "각 대화마다 \"Entering new ConversationChain chain...\"과 \"Finished chain.\" \n",
    "메시지가 표시되어 있어, 대화 체인이 3번 실행되었음을 알 수 있습니다.\n",
    "\n",
    "마지막으로, 대화 요약이 출력되었습니다. 이는 `memory.load_memory_variables({})`를 통해 저장된 대화 내용을 불러온 것입니다.\n",
    "\n",
    "이 시스템의 장점은 대화 내용을 요약하여 기억하기 때문에, 이전 대화 맥락을 유지하면서 응답할 수 있다는 것입니다. \n",
    "예를 들어, 세 번째 질문에서 AI가 이전에 무엇에 대해 물어봤는지 정확히 기억하고 있는 것을 볼 수 있습니다.\n",
    "\"\"\"\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    MessagesPlaceholder\n",
    ")\n",
    "\n",
    "# ChatOpenAI 인스턴스 생성\n",
    "llm = ChatOpenAI(temperature=0.7)\n",
    "\n",
    "# 메모리 설정\n",
    "memory = ConversationSummaryMemory(llm=llm, return_messages=True)\n",
    "\n",
    "# 프롬프트 템플릿 정의\n",
    "system_message = \"당신은 친절하고 도움이 되는 AI 어시스턴트입니다. 사용자와의 대화를 기억하고 이전 대화 내용을 바탕으로 응답합니다.\"\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(system_message),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"{input}\")\n",
    "])\n",
    "\n",
    "# 대화 체인 생성\n",
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory=memory,\n",
    "    prompt=prompt,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "def chat_with_bot(input_text):\n",
    "    response = conversation.predict(input=input_text)\n",
    "    print(f\"Human: {input_text}\")\n",
    "    print(f\"AI: {response}\\n\")\n",
    "\n",
    "# 사용 예시\n",
    "chat_with_bot(\"안녕하세요! 오늘 기분이 어떠세요?\")\n",
    "chat_with_bot(\"AI의 발전에 대해 어떻게 생각하시나요?\")\n",
    "chat_with_bot(\"제가 아까 무엇에 대해 물어봤었나요?\")\n",
    "\n",
    "# 대화 요약 출력\n",
    "print(\"대화 요약:\")\n",
    "print(memory.load_memory_variables({})[\"history\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'invoke' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 37\u001b[0m\n\u001b[1;32m     30\u001b[0m     memory\u001b[38;5;241m.\u001b[39msave_context(\n\u001b[1;32m     31\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m: question},\n\u001b[1;32m     32\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m: result\u001b[38;5;241m.\u001b[39mcontent},\n\u001b[1;32m     33\u001b[0m     )\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28mprint\u001b[39m(result)\n\u001b[0;32m---> 37\u001b[0m \u001b[43minvoke\u001b[49m(question\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m탑건\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'invoke' is not defined"
     ]
    }
   ],
   "source": [
    "# 과제\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "llm = ChatOpenAI(temperature=1) # 창의력을 높이기 위해 적정 값 1 적용\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=120, \n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"너는 질문에 대한 답변을 이모지 3개만을 활용해서 만들어 낼거야\"),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),  # 메모리 히스토리를 옮겨옴\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "def load_memory(_):\n",
    "    return memory.load_memory_variables({})[\"history\"]\n",
    "\n",
    "chain = RunnablePassthrough.assign(history=load_memory) | prompt | llm\n",
    "\n",
    "def invoke_chain(question):\n",
    "    result = chain.invoke({\"question\": question})\n",
    "    memory.save_context(\n",
    "        {\"input\": question},\n",
    "        {\"output\": result.content},\n",
    "    )\n",
    "    print(result)\n",
    "    \n",
    "\n",
    "invoke_chain(\"탑건\")\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
