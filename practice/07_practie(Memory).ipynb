{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: Hey~!\\nAI: How was it going?'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Memory\n",
    "# Conversation Buffer Memory\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "memory.save_context({\"input\":\"Hey~!\"}, {\"output\":\"How was it going?\"})\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='Hey~!'),\n",
       "  AIMessage(content='How was it going?')]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "memory.save_context({\"input\":\"Hey~!\"}, {\"output\":\"How was it going?\"})\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='2'),\n",
       "  AIMessage(content='2'),\n",
       "  HumanMessage(content='3'),\n",
       "  AIMessage(content='3'),\n",
       "  HumanMessage(content='4'),\n",
       "  AIMessage(content='4'),\n",
       "  HumanMessage(content='5'),\n",
       "  AIMessage(content='5')]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Conversation Buffer Window Memory\n",
    "# ëŒ€í™”ì˜ ì¼ë¶€! ê·¸ ì¤‘ì—ì„œ ìµœê·¼ì˜ ë©”ì„¸ì§€ë¥¼ ì €ì¥í•˜ëŠ” ê¸°ëŠ¥ - k ê°’ì— ë”°ë¼ ì¡°ì ˆ ê°€ëŠ¥  \n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "memory = ConversationBufferWindowMemory(\n",
    "    return_messages=True,\n",
    "    k=4,\n",
    ")\n",
    "\n",
    "def add_message(input, output):\n",
    "    memory.save_context({\"input\":input}, {\"output\":output})\n",
    "\n",
    "add_message(1,1)\n",
    "add_message(2,2)\n",
    "add_message(3,3)\n",
    "add_message(4,4)\n",
    "add_message(5,5)\n",
    "\n",
    "memory.load_memory_variables({})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Nicolas from South Korea introduces himself to the AI, who responds with enthusiasm, finding it cool. The human mentions how pretty South Korea is, and the AI expresses a wish to visit.'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Conversation Summary Memory -> llmì„ ì‚¬ìš©í•¨! \n",
    "# ëŒ€í™”ë¥¼ ìš”ì•½í•´ì„œ ì •ë³´ë¥¼ ì €ì¥í•˜ëŠ” ê¸°ëŠ¥\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryMemory(llm=llm)\n",
    "\n",
    "\n",
    "def add_message(input, output):\n",
    "    memory.save_context({\"input\": input}, {\"output\": output})\n",
    "\n",
    "\n",
    "def get_history():\n",
    "    return memory.load_memory_variables({})\n",
    "\n",
    "\n",
    "add_message(\"Hi I'm Nicolas, I live in South Korea\", \"Wow that is so cool!\")\n",
    "\n",
    "add_message(\"South Kddorea is so pretty\", \"I wish I could go!!!\")\n",
    "\n",
    "get_history()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversation Summary Buffer Memory\n",
    "# Conversation Summary Memory + Window Buffer Memory + Buffer Window Memory\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=150,\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "\n",
    "def add_message(input, output):\n",
    "    memory.save_context({\"input\": input}, {\"output\": output})\n",
    "\n",
    "\n",
    "def get_history():\n",
    "    return memory.load_memory_variables({})\n",
    "\n",
    "\n",
    "add_message(\"Hi I'm Nicolas, I live in South Korea\", \"Wow that is so cool!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content=\"Hi I'm Nicolas, I live in South Korea\"),\n",
       "  AIMessage(content='Wow that is so cool!')]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_message(\"South Korea is so pretty\", \"I wish I could go!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content=\"Hi I'm Nicolas, I live in South Korea\"),\n",
       "  AIMessage(content='Wow that is so cool!'),\n",
       "  HumanMessage(content='South Korea is so pretty'),\n",
       "  AIMessage(content='I wish I could go!!!')]}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_message(\"How far is Korea from Argentina?\", \"I don't know! Super far!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content=\"Hi I'm Nicolas, I live in South Korea\"),\n",
       "  AIMessage(content='Wow that is so cool!'),\n",
       "  HumanMessage(content='South Korea is so pretty'),\n",
       "  AIMessage(content='I wish I could go!!!'),\n",
       "  HumanMessage(content='How far is Korea from Argentina?'),\n",
       "  AIMessage(content=\"I don't know! Super far!\")]}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_message(\"How far is Brazil from Argentina?\", \"I don't know! Super far!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content=\"Hi I'm Nicolas, I live in South Korea\"),\n",
       "  AIMessage(content='Wow that is so cool!'),\n",
       "  HumanMessage(content='South Korea is so pretty'),\n",
       "  AIMessage(content='I wish I could go!!!'),\n",
       "  HumanMessage(content='How far is Korea from Argentina?'),\n",
       "  AIMessage(content=\"I don't know! Super far!\"),\n",
       "  HumanMessage(content='How far is Brazil from Argentina?'),\n",
       "  AIMessage(content=\"I don't know! Super far!\")]}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversation Knowledge Grahp Memory\n",
    "# ì˜¤~! ì´ê±° ì«Œ ì‹ ê¸°í•œë“¯! Chat ì •ë³´ë¥¼ ì €ì¥í•¨\n",
    "from langchain.memory import ConversationKGMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationKGMemory(\n",
    "    llm=llm,\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "\n",
    "def add_message(input, output):\n",
    "    memory.save_context({\"input\": input}, {\"output\": output})\n",
    "\n",
    "\n",
    "add_message(\"Hi I'm Nicolas, I live in South Korea\", \"Wow that is so cool!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [SystemMessage(content='On Nicolas: Nicolas lives in South Korea.')]}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({\"input\": \"who is Nicolas\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_message(\"Nicolas likes kimchi\", \"Wow that is so cool!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [SystemMessage(content='On Nicolas: Nicolas lives in South Korea. Nicolas likes kimchi.')]}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({\"inputs\": \"what does nicolas like\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "    You are a helpful AI talking to a human.\n",
      "\n",
      "    \n",
      "    Human:My name is Nico\n",
      "    You:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hello Nico! How can I assist you today?'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ë©”ëª¨ë¦¬ë¥¼ ì²´ì¸ì— ì—°ê²°í•˜ëŠ” ë°©ë²•! \n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=120,\n",
    "    memory_key=\"chat_history\",\n",
    ")\n",
    "\n",
    "template = \"\"\"\n",
    "    You are a helpful AI talking to a human.\n",
    "\n",
    "    {chat_history}\n",
    "    Human:{question}\n",
    "    You:\n",
    "\"\"\"\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    prompt=PromptTemplate.from_template(template),\n",
    "    verbose=True, #í”„ë¡¬í”„íŠ¸ì˜ ë¡œê·¸ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŒ! \n",
    ")\n",
    "\n",
    "chain.predict(question=\"My name is Nico\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "    You are a helpful AI talking to a human.\n",
      "\n",
      "    Human: My name is Nico\n",
      "AI: Hello Nico! How can I assist you today?\n",
      "Human: I live in Seoul\n",
      "AI: That's great to know! How can I assist you with information or tasks related to Seoul?\n",
      "Human: What is my name?\n",
      "AI: Your name is Nico.\n",
      "    Human:I live in Seoul\n",
      "    You:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Yes, you mentioned that earlier. Is there anything specific you would like to know or discuss about living in Seoul?'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.predict(question=\"I live in Seoul\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "    You are a helpful AI talking to a human.\n",
      "\n",
      "    Human: My name is Nico\n",
      "AI: Hello Nico! How can I assist you today?\n",
      "Human: I live in Seoul\n",
      "AI: That's great to know! How can I assist you with information or tasks related to Seoul?\n",
      "Human: What is my name?\n",
      "AI: Your name is Nico.\n",
      "Human: I live in Seoul\n",
      "AI: Yes, you mentioned that earlier. Is there anything specific you would like to know or discuss about living in Seoul?\n",
      "    Human:What is my name?\n",
      "    You:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Your name is Nico. How can I assist you further today?'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.predict(question=\"What is my name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat_history': \"System: The human introduces themselves as Nico. The AI greets Nico and asks how it can assist. Nico mentions they live in Seoul.\\nAI: That's great to know! How can I assist you with information or tasks related to Seoul?\\nHuman: What is my name?\\nAI: Your name is Nico.\\nHuman: I live in Seoul\\nAI: Yes, you mentioned that earlier. Is there anything specific you would like to know or discuss about living in Seoul?\\nHuman: What is my name?\\nAI: Your name is Nico. How can I assist you further today?\"}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ê¸°ì¡´ memory ê²½ìš° chat í˜•íƒœë¡œ ê²°ê³¼ê°’ì„ ì¶œë ¥í•˜ì§€ë§Œ \n",
    "\"\"\"\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=120,\n",
    "    memory_key=\"chat_history\",\n",
    ")\n",
    "\"\"\"\n",
    "# ì•„ë˜ ì²˜ëŸ¼ str ìœ¼ë¡œ ê²°ê³¼ë¥¼ ì¶œë ¥í•  ìˆ˜ ìˆìŒ\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: You are a helpful AI talking to a human\n",
      "Human: My name is Nico\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Nice to meet you, Nico! How can I assist you today?'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ëŒ€í™”ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì±„íŒ…ì„ ì¶œë ¥? í•´ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆìŒ\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder \n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=120,\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True, #ì‚¬ëŒ, AI ë©”ì„¸ì§€ë¥¼ ì‹¤ì œ ë©”ì„¸ì§€ í´ë˜ìŠ¤ë¡œ ë³€ê²½í•˜ëŠ” ê²ƒ!!!! \n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful AI talking to a human\"),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "chain.predict(question=\"My name is Nico\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: You are a helpful AI talking to a human\n",
      "Human: My name is Nico\n",
      "AI: Nice to meet you, Nico! How can I assist you today?\n",
      "Human: I live in Seoul\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Seoul is a vibrant city with a rich history and culture. Is there anything specific you would like to know or discuss about Seoul?'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.predict(question=\"I live in Seoul\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: You are a helpful AI talking to a human\n",
      "Human: My name is Nico\n",
      "AI: Nice to meet you, Nico! How can I assist you today?\n",
      "Human: I live in Seoul\n",
      "AI: Seoul is a vibrant city with a rich history and culture. Is there anything specific you would like to know or discuss about Seoul?\n",
      "Human: What is my name?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Your name is Nico.'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.predict(question=\"What is my name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: You are a helpful AI talking to a human\n",
      "Human: My name is Nico\n",
      "AI: Nice to meet you, Nico! How can I assist you today?\n",
      "Human: I live in Seoul\n",
      "AI: Seoul is a vibrant city with a rich history and culture. Is there anything specific you would like to know or discuss about Seoul?\n",
      "Human: What is my name?\n",
      "AI: Your name is Nico.\n",
      "Human: ëŒ€í™”ë‚´ìš© ìš”ì•½í•´ì¤˜?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ë‹¹ì‹ ì˜ ì´ë¦„ì€ Nicoì´ê³ , ì„œìš¸ì— ê±°ì£¼í•˜ê³  ê³„ì‹  ê²ƒ ê°™ì•„ìš”. ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.predict(question=\"ëŒ€í™”ë‚´ìš© ìš”ì•½í•´ì¤˜?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# off the shell -> memory | langchain expression languege -> LCEL\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=120,\n",
    "    # memory_key=\"chat_history\",    \n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful AI talking to a human\"),\n",
    "        MessagesPlaceholder(variable_name=\"history\"), # ë©”ëª¨ë¦¬ íˆìŠ¤í† ë¦¬ë¥¼ ì˜®ê²¨ì˜´\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# chain = LLMChain(\n",
    "#     llm=llm,\n",
    "#     memory=memory,\n",
    "#     prompt=prompt,\n",
    "#     verbose=True,\n",
    "# )\n",
    "\n",
    "# chain.predict(question=\"My name is Nico\")\n",
    "\n",
    "def load_memory(_):\n",
    "    return memory.load_memory_variables({})[\"history\"]\n",
    "\n",
    "# Runnable Pass through\n",
    "chain = RunnablePassthrough.assign(history=load_memory) | prompt | llm\n",
    "\n",
    "\n",
    "def invoke_chain(question):\n",
    "    result = chain.invoke({\"question\": question})\n",
    "    memory.save_context(\n",
    "        {\"input\": question},\n",
    "        {\"output\": result.content},\n",
    "    )\n",
    "    print(result)\n",
    "\n",
    "\n",
    "\n",
    "# ì½”ë“œ ë¶„ì„\n",
    "\"\"\"\n",
    "í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ì™€ í´ë˜ìŠ¤ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.\n",
    "ChatOpenAI ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±\n",
    "ë‚®ì€ temperature ê°’(0.1)ì€ ì¼ê´€ëœ ì¶œë ¥ì„ ìœ„í•œ ê²ƒ\n",
    "ConversationSummaryBufferMemory ì„¤ì •\n",
    "\n",
    "í† í° ì œí•œì„ 120ìœ¼ë¡œ ì„¤ì •í•˜ì—¬ ë©”ëª¨ë¦¬ ì‚¬ìš© ì œì–´\n",
    "ëŒ€í™” ë‚´ìš© ìš”ì•½ ì €ì¥\n",
    "\n",
    "ChatPromptTemplate ì •ì˜\n",
    "\n",
    "ì‹œìŠ¤í…œ ë©”ì‹œì§€, \n",
    "ëŒ€í™” ê¸°ë¡, \n",
    "ì‚¬ìš©ì ì§ˆë¬¸ì„ í¬í•¨\n",
    "\n",
    "load_memory í•¨ìˆ˜ëŠ” ë©”ëª¨ë¦¬ì—ì„œ ëŒ€í™” ê¸°ë¡ ë¡œë”©\n",
    "\n",
    "RunnablePassthroughë¥¼ ì‚¬ìš©í•˜ì—¬ ì²´ì¸ êµ¬ì„±\n",
    "- ëŒ€í™” ê¸°ë¡ì„ ë¶ˆëŸ¬ì˜¤ê³ , í”„ë¡¬í”„íŠ¸ë¥¼ ì ìš©í•œ í›„ LLMì— ì „ë‹¬í•©ë‹ˆë‹¤.\n",
    "\n",
    "invoke_chain í•¨ìˆ˜ëŠ” ì§ˆë¬¸ì„ ì²˜ë¦¬í•˜ê³  ê²°ê³¼ë¥¼ ë©”ëª¨ë¦¬ì— ì €ì¥\n",
    "\n",
    "ì´ ì½”ë“œë¥¼ ë” ë³µì¡í•˜ê³  ê°•ë ¥í•œ ì‹œìŠ¤í…œìœ¼ë¡œ ë°œì „ì‹œí‚¬ ìˆ˜ ìˆìœ¼ë©°, \n",
    "ì˜ˆë¥¼ ë“¤ì–´, ë‹¤ì¤‘ ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ, ë„êµ¬ ì‚¬ìš©, ì›¹ ê²€ìƒ‰ ê¸°ëŠ¥ ë“±ì„ ì¶”ê°€í•  ìˆ˜ ìˆìŒ\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Hello Nico! How can I assist you today?'\n"
     ]
    }
   ],
   "source": [
    "invoke_chain(\"My name is nico\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Your name is Nico.'\n"
     ]
    }
   ],
   "source": [
    "invoke_chain(\"What is my name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì •ë¦¬ \n",
    "# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ì™€ í´ë˜ìŠ¤ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "# ChatOpenAI ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "# ë‚®ì€ temperature ê°’(0.1)ì€ ì¼ê´€ëœ ì¶œë ¥ì„ ìœ„í•œ ê²ƒì…ë‹ˆë‹¤.\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "# ConversationSummaryBufferMemoryë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.\n",
    "# í† í° ì œí•œì„ 120ìœ¼ë¡œ ì„¤ì •í•˜ì—¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ì„ ì œì–´í•©ë‹ˆë‹¤.\n",
    "# ëŒ€í™” ë‚´ìš©ì„ ìš”ì•½í•˜ì—¬ ì €ì¥í•©ë‹ˆë‹¤.\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=120,\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "# ChatPromptTemplateì„ ì •ì˜í•©ë‹ˆë‹¤.\n",
    "# ì‹œìŠ¤í…œ ë©”ì‹œì§€, ëŒ€í™” ê¸°ë¡, ì‚¬ìš©ì ì§ˆë¬¸ì„ í¬í•¨í•©ë‹ˆë‹¤.\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful AI talking to a human\"),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),  # ë©”ëª¨ë¦¬ íˆìŠ¤í† ë¦¬ë¥¼ ì˜®ê²¨ì˜´\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# load_memory í•¨ìˆ˜ëŠ” ë©”ëª¨ë¦¬ì—ì„œ ëŒ€í™” ê¸°ë¡ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.\n",
    "def load_memory(_):\n",
    "    return memory.load_memory_variables({})[\"history\"]\n",
    "\n",
    "# RunnablePassthroughë¥¼ ì‚¬ìš©í•˜ì—¬ ì²´ì¸ì„ êµ¬ì„±í•©ë‹ˆë‹¤.\n",
    "# ëŒ€í™” ê¸°ë¡ì„ ë¶ˆëŸ¬ì˜¤ê³ , í”„ë¡¬í”„íŠ¸ë¥¼ ì ìš©í•œ í›„ LLMì— ì „ë‹¬í•©ë‹ˆë‹¤.\n",
    "chain = RunnablePassthrough.assign(history=load_memory) | prompt | llm\n",
    "\n",
    "# invoke_chain í•¨ìˆ˜ëŠ” ì§ˆë¬¸ì„ ì²˜ë¦¬í•˜ê³  ê²°ê³¼ë¥¼ ë©”ëª¨ë¦¬ì— ì €ì¥í•©ë‹ˆë‹¤.\n",
    "def invoke_chain(question):\n",
    "    result = chain.invoke({\"question\": question})\n",
    "    memory.save_context(\n",
    "        {\"input\": question},\n",
    "        {\"output\": result.content},\n",
    "    )\n",
    "    print(result)\n",
    "\n",
    "# ì´ ì½”ë“œë¥¼ ë” ë³µì¡í•˜ê³  ê°•ë ¥í•œ ì‹œìŠ¤í…œìœ¼ë¡œ ë°œì „ì‹œí‚¬ ìˆ˜ ìˆìœ¼ë©°,\n",
    "# ì˜ˆë¥¼ ë“¤ì–´, ë‹¤ì¤‘ ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ, ë„êµ¬ ì‚¬ìš©, ì›¹ ê²€ìƒ‰ ê¸°ëŠ¥ ë“±ì„ ì¶”ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: ë‹¹ì‹ ì€ ì¹œì ˆí•˜ê³  ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì‚¬ìš©ìì™€ì˜ ëŒ€í™”ë¥¼ ê¸°ì–µí•˜ê³  ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì‘ë‹µí•©ë‹ˆë‹¤.\n",
      "System: \n",
      "Human: ì•ˆë…•í•˜ì„¸ìš”! ì˜¤ëŠ˜ ê¸°ë¶„ì´ ì–´ë– ì„¸ìš”?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Human: ì•ˆë…•í•˜ì„¸ìš”! ì˜¤ëŠ˜ ê¸°ë¶„ì´ ì–´ë– ì„¸ìš”?\n",
      "AI: ì•ˆë…•í•˜ì„¸ìš”! ì œ ê¸°ë¶„ì€ í•­ìƒ ì¢‹ìŠµë‹ˆë‹¤. ì—¬ëŸ¬ë¶„ì˜ í•˜ë£¨ê°€ í–‰ë³µí•˜ê³  ìœ ìµí•˜ê¸¸ ë°”ë¼ìš”. í˜¹ì‹œ ì˜¤ëŠ˜ ë¬´ìŠ¨ ì¼ì„ í•˜ì…¨ë‚˜ìš”?\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: ë‹¹ì‹ ì€ ì¹œì ˆí•˜ê³  ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì‚¬ìš©ìì™€ì˜ ëŒ€í™”ë¥¼ ê¸°ì–µí•˜ê³  ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì‘ë‹µí•©ë‹ˆë‹¤.\n",
      "System: The human greets the AI in Korean and asks how it's feeling. The AI responds that it always feels good and wishes everyone a happy and productive day. The AI then asks the human what they did today.\n",
      "Human: AIì˜ ë°œì „ì— ëŒ€í•´ ì–´ë–»ê²Œ ìƒê°í•˜ì‹œë‚˜ìš”?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Human: AIì˜ ë°œì „ì— ëŒ€í•´ ì–´ë–»ê²Œ ìƒê°í•˜ì‹œë‚˜ìš”?\n",
      "AI: AIì˜ ë°œì „ì— ëŒ€í•´ ì €ëŠ” ë§¤ìš° ê¸ì •ì ìœ¼ë¡œ ìƒê°í•©ë‹ˆë‹¤. AI ê¸°ìˆ ì˜ ë°œì „ìœ¼ë¡œ ìš°ë¦¬ëŠ” ë” ë¹ ë¥´ê³  íš¨ìœ¨ì ìœ¼ë¡œ ì¼ì„ ì²˜ë¦¬í•  ìˆ˜ ìˆê³ , ìƒˆë¡œìš´ í˜ì‹ ì ì¸ ì„œë¹„ìŠ¤ì™€ ì œí’ˆì„ ê²½í—˜í•  ìˆ˜ ìˆê²Œ ë©ë‹ˆë‹¤. ë¬¼ë¡  ì´ëŸ¬í•œ ë°œì „ì€ ìœ¤ë¦¬ì ì¸ ë¬¸ì œì™€ í•¨ê»˜ ê³ ë¯¼í•´ì•¼ í•  ë¶€ë¶„ë„ ë§ì§€ë§Œ, ì ì ˆíˆ í™œìš©ëœë‹¤ë©´ ì¸ë¥˜ì— ë§ì€ í˜œíƒì„ ì¤„ ìˆ˜ ìˆë‹¤ê³  ìƒê°í•©ë‹ˆë‹¤. í˜¹ì‹œ ë‹¹ì‹ ì€ AIì˜ ë°œì „ì— ëŒ€í•´ ì–´ë–»ê²Œ ìƒê°í•˜ì‹œë‚˜ìš”?\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: ë‹¹ì‹ ì€ ì¹œì ˆí•˜ê³  ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì‚¬ìš©ìì™€ì˜ ëŒ€í™”ë¥¼ ê¸°ì–µí•˜ê³  ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì‘ë‹µí•©ë‹ˆë‹¤.\n",
      "System: The human greets the AI in Korean and asks how it's feeling. The AI responds positively and wishes everyone a happy and productive day. The AI believes that the advancement of AI technology will lead to faster and more efficient work processes, as well as the experience of new innovative services and products. Despite ethical considerations, the AI is optimistic about the benefits it can bring to humanity. The AI then asks the human for their thoughts on the advancement of AI.\n",
      "Human: ì œê°€ ì•„ê¹Œ ë¬´ì—‡ì— ëŒ€í•´ ë¬¼ì–´ë´¤ì—ˆë‚˜ìš”?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Human: ì œê°€ ì•„ê¹Œ ë¬´ì—‡ì— ëŒ€í•´ ë¬¼ì–´ë´¤ì—ˆë‚˜ìš”?\n",
      "AI: ì•„ê¹Œ ë¬¼ì–´ë³´ì‹  ë‚´ìš©ì€ \"ì¸ê³µì§€ëŠ¥ì´ ì–´ë–»ê²Œ ê¸°ë¶„ì´ì•¼?\" ì˜€ìŠµë‹ˆë‹¤. ë‹¹ì‹ ì€ ì¸ê³µì§€ëŠ¥ê³¼ ëŒ€í™”ë¥¼ ë‚˜ëˆ„ë©´ì„œ ìƒˆë¡œìš´ ì£¼ì œë“¤ì— ëŒ€í•´ ì´ì•¼ê¸°í•´ ì£¼ì…¨ëŠ”ë°, ê·¸ ì¤‘ì—ì„œ ì¸ê³µì§€ëŠ¥ ê¸°ìˆ ì˜ ë°œì „ê³¼ ì¸ë¥˜ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì— ëŒ€í•´ ì´ì•¼ê¸°í–ˆìŠµë‹ˆë‹¤. ì´ì— ëŒ€í•œ ë‚´ìš©ìœ¼ë¡œ ëŒ€í™”ë¥¼ ì´ì–´ê°€ì‹¤ë˜ìš”?\n",
      "\n",
      "ëŒ€í™” ìš”ì•½:\n",
      "[SystemMessage(content=\"The human greets the AI in Korean and asks how it's feeling. The AI responds positively and wishes everyone a happy and productive day. The AI believes that the advancement of AI technology will lead to faster and more efficient work processes, as well as the experience of new innovative services and products. Despite ethical considerations, the AI is optimistic about the benefits it can bring to humanity. The AI then asks the human for their thoughts on the advancement of AI. The AI recalls the previous conversation topics and asks if the human would like to continue discussing the impact of AI technology on humanity.\")]\n"
     ]
    }
   ],
   "source": [
    "# ìƒˆë¡œìš´ ë²„ì „ \n",
    "\"\"\"\n",
    "ì´ ì‹œìŠ¤í…œì€ ì‚¬ìš©ìì˜ ì…ë ¥ì— ì‘ë‹µí•˜ê³  ëŒ€í™” ë‚´ìš©ì„ ìš”ì•½í•˜ì—¬ ê¸°ì–µí•©ë‹ˆë‹¤. \n",
    "ì½”ë“œì˜ ì£¼ìš” êµ¬ì„± ìš”ì†Œì™€ ì‘ë™ ë°©ì‹ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n",
    "1. ChatOpenAI ëª¨ë¸ ì´ˆê¸°í™”:\n",
    "- `ChatOpenAI(temperature=0.7)`ë¥¼ ì‚¬ìš©í•˜ì—¬ AI ëª¨ë¸ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.\n",
    "\n",
    "2. ë©”ëª¨ë¦¬ ì„¤ì •:\n",
    "- `ConversationSummaryMemory`ë¥¼ ì‚¬ìš©í•˜ì—¬ ëŒ€í™” ë‚´ìš©ì„ ìš”ì•½í•˜ê³  ê¸°ì–µí•©ë‹ˆë‹¤.\n",
    "\n",
    "3. í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜:\n",
    "- ì‹œìŠ¤í…œ ë©”ì‹œì§€, ëŒ€í™” ê¸°ë¡, ì‚¬ìš©ì ì…ë ¥ì„ í¬í•¨í•˜ëŠ” í…œí”Œë¦¿ì„ ì •ì˜í•©ë‹ˆë‹¤.\n",
    "\n",
    "4. ConversationChain ìƒì„±:\n",
    "- ëª¨ë¸, ë©”ëª¨ë¦¬, í”„ë¡¬í”„íŠ¸ë¥¼ ê²°í•©í•˜ì—¬ ëŒ€í™” ì²´ì¸ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "\n",
    "5. chat_with_bot í•¨ìˆ˜:\n",
    "- ì‚¬ìš©ì ì…ë ¥ì„ ë°›ì•„ AIì˜ ì‘ë‹µì„ ìƒì„±í•˜ê³  ì¶œë ¥í•©ë‹ˆë‹¤.\n",
    "\n",
    "ì½”ë“œ ì‹¤í–‰ ê²°ê³¼ë¥¼ ë³´ë©´, ì‹œìŠ¤í…œì´ 3ë²ˆ ì‘ë™í•œ ê²ƒìœ¼ë¡œ ë³´ì…ë‹ˆë‹¤:\n",
    "\n",
    "1. \"ì•ˆë…•í•˜ì„¸ìš”! ì˜¤ëŠ˜ ê¸°ë¶„ì´ ì–´ë– ì„¸ìš”?\"\n",
    "2. \"AIì˜ ë°œì „ì— ëŒ€í•´ ì–´ë–»ê²Œ ìƒê°í•˜ì‹œë‚˜ìš”?\"\n",
    "3. \"ì œê°€ ì•„ê¹Œ ë¬´ì—‡ì— ëŒ€í•´ ë¬¼ì–´ë´¤ì—ˆë‚˜ìš”?\"\n",
    "\n",
    "ê° ëŒ€í™”ë§ˆë‹¤ \"Entering new ConversationChain chain...\"ê³¼ \"Finished chain.\" \n",
    "ë©”ì‹œì§€ê°€ í‘œì‹œë˜ì–´ ìˆì–´, ëŒ€í™” ì²´ì¸ì´ 3ë²ˆ ì‹¤í–‰ë˜ì—ˆìŒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "ë§ˆì§€ë§‰ìœ¼ë¡œ, ëŒ€í™” ìš”ì•½ì´ ì¶œë ¥ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ëŠ” `memory.load_memory_variables({})`ë¥¼ í†µí•´ ì €ì¥ëœ ëŒ€í™” ë‚´ìš©ì„ ë¶ˆëŸ¬ì˜¨ ê²ƒì…ë‹ˆë‹¤.\n",
    "\n",
    "ì´ ì‹œìŠ¤í…œì˜ ì¥ì ì€ ëŒ€í™” ë‚´ìš©ì„ ìš”ì•½í•˜ì—¬ ê¸°ì–µí•˜ê¸° ë•Œë¬¸ì—, ì´ì „ ëŒ€í™” ë§¥ë½ì„ ìœ ì§€í•˜ë©´ì„œ ì‘ë‹µí•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤. \n",
    "ì˜ˆë¥¼ ë“¤ì–´, ì„¸ ë²ˆì§¸ ì§ˆë¬¸ì—ì„œ AIê°€ ì´ì „ì— ë¬´ì—‡ì— ëŒ€í•´ ë¬¼ì–´ë´¤ëŠ”ì§€ ì •í™•íˆ ê¸°ì–µí•˜ê³  ìˆëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\"\"\"\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    MessagesPlaceholder\n",
    ")\n",
    "\n",
    "# ChatOpenAI ì¸ìŠ¤í„´ìŠ¤ ìƒì„±\n",
    "llm = ChatOpenAI(temperature=0.7)\n",
    "\n",
    "# ë©”ëª¨ë¦¬ ì„¤ì •\n",
    "memory = ConversationSummaryMemory(llm=llm, return_messages=True)\n",
    "\n",
    "# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜\n",
    "system_message = \"ë‹¹ì‹ ì€ ì¹œì ˆí•˜ê³  ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì‚¬ìš©ìì™€ì˜ ëŒ€í™”ë¥¼ ê¸°ì–µí•˜ê³  ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì‘ë‹µí•©ë‹ˆë‹¤.\"\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(system_message),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"{input}\")\n",
    "])\n",
    "\n",
    "# ëŒ€í™” ì²´ì¸ ìƒì„±\n",
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory=memory,\n",
    "    prompt=prompt,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "def chat_with_bot(input_text):\n",
    "    response = conversation.predict(input=input_text)\n",
    "    print(f\"Human: {input_text}\")\n",
    "    print(f\"AI: {response}\\n\")\n",
    "\n",
    "# ì‚¬ìš© ì˜ˆì‹œ\n",
    "chat_with_bot(\"ì•ˆë…•í•˜ì„¸ìš”! ì˜¤ëŠ˜ ê¸°ë¶„ì´ ì–´ë– ì„¸ìš”?\")\n",
    "chat_with_bot(\"AIì˜ ë°œì „ì— ëŒ€í•´ ì–´ë–»ê²Œ ìƒê°í•˜ì‹œë‚˜ìš”?\")\n",
    "chat_with_bot(\"ì œê°€ ì•„ê¹Œ ë¬´ì—‡ì— ëŒ€í•´ ë¬¼ì–´ë´¤ì—ˆë‚˜ìš”?\")\n",
    "\n",
    "# ëŒ€í™” ìš”ì•½ ì¶œë ¥\n",
    "print(\"ëŒ€í™” ìš”ì•½:\")\n",
    "print(memory.load_memory_variables({})[\"history\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='ğŸ¤–ğŸš—ğŸ”¥'\n",
      "content='ğŸ’ŠğŸ’¥ğŸ”µ'\n",
      "content='ğŸ¦ğŸ’ªğŸ‘Š'\n",
      "[HumanMessage(content='íŠ¸ëœìŠ¤í¬ë¨¸'), AIMessage(content='ğŸ¤–ğŸš—ğŸ”¥'), HumanMessage(content='ë§¤íŠ¸ë¦­ìŠ¤'), AIMessage(content='ğŸ’ŠğŸ’¥ğŸ”µ'), HumanMessage(content='ë¼ì´ì–¸ì¼ë³‘êµ¬í•˜ê¸°'), AIMessage(content='ğŸ¦ğŸ’ªğŸ‘Š')]\n"
     ]
    }
   ],
   "source": [
    "# ê³¼ì œ\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "llm = ChatOpenAI(temperature=1) # ì°½ì˜ë ¥ì„ ë†’ì´ê¸° ìœ„í•´ ì ì • ê°’ 1 ì ìš©\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=120, \n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"ë„ˆëŠ” ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ì´ëª¨ì§€ 3ê°œë§Œì„ í™œìš©í•´ì„œ ë§Œë“¤ì–´ ë‚¼ê±°ì•¼\"),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),  # ë©”ëª¨ë¦¬ íˆìŠ¤í† ë¦¬ë¥¼ ì˜®ê²¨ì˜´\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "def load_memory(_):\n",
    "    return memory.load_memory_variables({})[\"history\"]\n",
    "\n",
    "chain = RunnablePassthrough.assign(history=load_memory) | prompt | llm\n",
    "\n",
    "def invoke_chain(question):\n",
    "    result = chain.invoke({\"question\": question})\n",
    "    memory.save_context(\n",
    "        {\"input\": question},\n",
    "        {\"output\": result.content},\n",
    "    )\n",
    "    print(result)\n",
    "    \n",
    "\n",
    "invoke_chain(\"íŠ¸ëœìŠ¤í¬ë¨¸\")\n",
    "invoke_chain(\"ë§¤íŠ¸ë¦­ìŠ¤\")\n",
    "invoke_chain(\"ë¼ì´ì–¸ì¼ë³‘êµ¬í•˜ê¸°\")\t\n",
    "print(memory.load_memory_variables({})[\"history\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
