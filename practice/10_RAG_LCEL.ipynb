{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Victory Mansions is a building where Winston Smith resides. It is a run-down apartment complex with a faulty lift, gritty dust, and a hallway that smells of boiled cabbage and old rag mats. The building is affected by power cuts during daylight hours as part of an economy drive. A large colored poster with the face of a man about forty-five years old, with a heavy black mustache, is displayed on one wall. Winston\\'s flat is on the seventh floor, and the building is adorned with posters that read \"BIG BROTHER IS WATCHING YOU.\"')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#RunnablePassthrough(), \n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    ")\n",
    "\n",
    "cache_dir = LocalFileStore(\"./.cache/\")\n",
    "\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "loader = UnstructuredFileLoader(\"./rag_data/chapter_one.txt\")\n",
    "\n",
    "docs = loader.load_and_split(text_splitter=splitter)\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "cached_embeddings = CacheBackedEmbeddings.from_bytes_store(embeddings, cache_dir)\n",
    "\n",
    "vectorstore = FAISS.from_documents(docs, cached_embeddings)\n",
    "\n",
    "retriver = vectorstore.as_retriever()\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant. Answer questions using only the following context. If you don't know the answer just say you don't know, don't make it up:\\n\\n{context}\",\n",
    "        ),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        \"context\": retriver,\n",
    "        \"question\": RunnablePassthrough(),\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    ")\n",
    "\n",
    "chain.invoke(\"Describe Victory Mansions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='주인공은 윈스턴 스미스입니다.')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"주인공이 누구야?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='죄송합니다. 알려드릴 수 있는 정보가 없습니다.')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"왜?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='죄송합니다. 그 정보는 제가 알고 있는 내용에는 포함되어 있지 않습니다.')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"왜 주인공이 윈스턴 스미스야?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Three ministries are mentioned in the text: the Ministry of Love, the Ministry of Plenty, and the Ministry of Truth.')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Map Reduce LCEL(LangChain Expression Languege) Chain | \n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    ")\n",
    "\n",
    "cache_dir = LocalFileStore(\"./.cache/\")\n",
    "\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "loader = UnstructuredFileLoader(\"./rag_data/chapter_one.txt\")\n",
    "\n",
    "docs = loader.load_and_split(text_splitter=splitter)\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "cached_embeddings = CacheBackedEmbeddings.from_bytes_store(embeddings, cache_dir)\n",
    "\n",
    "vectorstore = FAISS.from_documents(docs, cached_embeddings)\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "\n",
    "map_doc_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"\n",
    "            Use the following portion of a long document to see if any of the text is relevant to answer the question. Return any relevant text verbatim. If there is no relevant text, return : ''\n",
    "            -------\n",
    "            {context}\n",
    "            \"\"\",\n",
    "        ),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "map_doc_chain = map_doc_prompt | llm\n",
    "\n",
    "\n",
    "def map_docs(inputs):\n",
    "    documents = inputs[\"documents\"]\n",
    "    question = inputs[\"question\"]\n",
    "    return \"\\n\\n\".join(\n",
    "        map_doc_chain.invoke(\n",
    "            {\"context\": doc.page_content, \"question\": question}\n",
    "        ).content\n",
    "        for doc in documents\n",
    "    )\n",
    "\n",
    "\n",
    "map_chain = {\n",
    "    \"documents\": retriever,\n",
    "    \"question\": RunnablePassthrough(),\n",
    "} | RunnableLambda(map_docs)\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"\n",
    "            Given the following extracted parts of a long document and a question, create a final answer. \n",
    "            If you don't know the answer, just say that you don't know. Don't try to make up an answer.\n",
    "            ------\n",
    "            {context}\n",
    "            \"\"\",\n",
    "        ),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = {\"context\": map_chain, \"question\": RunnablePassthrough()} | final_prompt | llm\n",
    "\n",
    "chain.invoke(\"How many ministries are mentioned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Winston goes to work at the Ministry of Truth.')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"where dose Winsthon go to work\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Winston is a smallish, frail figure with fair hair, a naturally sanguine face, and roughened skin from coarse soap, blunt razor blades, and the cold winter weather. He wears blue overalls, the uniform of the party. Winston sets his features into an expression of quiet optimism when facing the telescreen, drinks Victory Gin, smokes Victory Cigarettes, and is drawn to a man named O'Brien. He feels uneasiness and fear mixed with hostility around a particular girl and experiences a constricted diaphragm when seeing the face of Goldstein.\")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"Describe Winston\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG 시스템 코드 분석\n",
    "\n",
    "## 1. 필요한 라이브러리 및 클래스 임포트\n",
    "\n",
    "```python\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
    "```\n",
    "\n",
    "이 부분에서는 LangChain 라이브러리에서 필요한 모든 클래스와 함수를 임포트합니다. 각 임포트의 역할은 다음과 같습니다:\n",
    "\n",
    "- `ChatOpenAI`: OpenAI의 챗봇 모델을 사용하기 위한 클래스\n",
    "- `UnstructuredFileLoader`: 구조화되지 않은 파일(텍스트 파일 등)을 로드하기 위한 클래스\n",
    "- `CharacterTextSplitter`: 텍스트를 작은 청크로 분할하기 위한 클래스\n",
    "- `OpenAIEmbeddings`, `CacheBackedEmbeddings`: 텍스트를 벡터로 변환하기 위한 클래스들\n",
    "- `FAISS`: 벡터 데이터베이스를 생성하고 관리하기 위한 클래스\n",
    "- `LocalFileStore`: 로컬 파일 시스템에 데이터를 저장하기 위한 클래스\n",
    "- `ChatPromptTemplate`: 챗봇 프롬프트를 만들기 위한 클래스\n",
    "- `RunnablePassthrough`, `RunnableLambda`: 데이터 처리 파이프라인을 구성하기 위한 클래스들\n",
    "\n",
    "## 2. 언어 모델 초기화\n",
    "\n",
    "```python\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    ")\n",
    "```\n",
    "\n",
    "OpenAI의 챗봇 모델을 초기화합니다. `temperature=0.1`로 설정하여 모델의 출력을 더 결정적(덜 무작위적)으로 만듭니다.\n",
    "\n",
    "## 3. 캐시 디렉토리 설정\n",
    "\n",
    "```python\n",
    "cache_dir = LocalFileStore(\"./.cache/\")\n",
    "```\n",
    "\n",
    "임베딩을 캐시하기 위한 로컬 디렉토리를 설정합니다. 이는 반복적인 계산을 줄이고 성능을 향상시키는 데 도움이 됩니다.\n",
    "\n",
    "## 4. 텍스트 분할기 설정\n",
    "\n",
    "```python\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "```\n",
    "\n",
    "텍스트를 작은 청크로 나누기 위한 분할기를 설정합니다. 여기서는:\n",
    "- 줄바꿈(`\\n`)을 기준으로 텍스트를 나눕니다.\n",
    "- 각 청크의 크기는 600 토큰입니다.\n",
    "- 청크 간 100 토큰의 중복을 허용하여 문맥의 연속성을 유지합니다.\n",
    "\n",
    "## 5. 문서 로드 및 분할\n",
    "\n",
    "```python\n",
    "loader = UnstructuredFileLoader(\"./rag_data/chapter_one.txt\")\n",
    "docs = loader.load_and_split(text_splitter=splitter)\n",
    "```\n",
    "\n",
    "`UnstructuredFileLoader`를 사용하여 텍스트 파일을 로드하고, 앞서 정의한 `splitter`를 사용하여 이를 작은 청크로 나눕니다.\n",
    "\n",
    "## 6. 임베딩 설정\n",
    "\n",
    "```python\n",
    "embeddings = OpenAIEmbeddings()\n",
    "cached_embeddings = CacheBackedEmbeddings.from_bytes_store(embeddings, cache_dir)\n",
    "```\n",
    "\n",
    "OpenAI의 임베딩 모델을 초기화하고, 이를 캐시 기반 임베딩으로 래핑합니다. 이렇게 하면 이전에 계산된 임베딩을 재사용할 수 있어 효율성이 향상됩니다.\n",
    "\n",
    "## 7. 벡터 데이터베이스 생성\n",
    "\n",
    "```python\n",
    "vectorstore = FAISS.from_documents(docs, cached_embeddings)\n",
    "```\n",
    "\n",
    "FAISS를 사용하여 문서의 벡터 표현을 저장하는 데이터베이스를 생성합니다. 이는 효율적인 유사성 검색을 가능하게 합니다.\n",
    "\n",
    "## 8. 검색기(Retriever) 설정\n",
    "\n",
    "```python\n",
    "retriever = vectorstore.as_retriever()\n",
    "```\n",
    "\n",
    "벡터 데이터베이스를 검색기로 변환합니다. 이를 통해 질문과 관련된 문서를 효율적으로 검색할 수 있습니다.\n",
    "\n",
    "## 9. 문서 매핑 프롬프트 설정\n",
    "\n",
    "```python\n",
    "map_doc_prompt = ChatPromptTemplate.from_messages([...])\n",
    "```\n",
    "\n",
    "각 문서 청크를 처리하기 위한 프롬프트 템플릿을 정의합니다. 이 프롬프트는 시스템과 사용자 메시지로 구성되며, 문서의 관련 부분을 추출하는 데 사용됩니다.\n",
    "\n",
    "## 10. 문서 매핑 체인 설정\n",
    "\n",
    "```python\n",
    "map_doc_chain = map_doc_prompt | llm\n",
    "```\n",
    "\n",
    "프롬프트와 언어 모델을 연결하여 각 문서 청크를 처리하는 체인을 생성합니다.\n",
    "\n",
    "## 11. 문서 매핑 함수 정의\n",
    "\n",
    "```python\n",
    "def map_docs(inputs):\n",
    "    ...\n",
    "```\n",
    "\n",
    "이 함수는 검색된 모든 문서에 대해 `map_doc_chain`을 적용하고 결과를 결합합니다.\n",
    "\n",
    "## 12. 매핑 체인 설정\n",
    "\n",
    "```python\n",
    "map_chain = {\n",
    "    \"documents\": retriever,\n",
    "    \"question\": RunnablePassthrough(),\n",
    "} | RunnableLambda(map_docs)\n",
    "```\n",
    "\n",
    "검색기와 `map_docs` 함수를 결합하여 전체 매핑 프로세스를 정의합니다.\n",
    "\n",
    "## 13. 최종 프롬프트 설정\n",
    "\n",
    "```python\n",
    "final_prompt = ChatPromptTemplate.from_messages([...])\n",
    "```\n",
    "\n",
    "최종 답변을 생성하기 위한 프롬프트 템플릿을 정의합니다.\n",
    "\n",
    "## 14. 전체 체인 구성\n",
    "\n",
    "```python\n",
    "chain = {\"context\": map_chain, \"question\": RunnablePassthrough()} | final_prompt | llm\n",
    "```\n",
    "\n",
    "모든 구성 요소를 하나의 체인으로 결합합니다. 이 체인은:\n",
    "1. 질문을 받습니다.\n",
    "2. 관련 문서를 검색합니다.\n",
    "3. 검색된 문서에서 관련 정보를 추출합니다.\n",
    "4. 최종 답변을 생성합니다.\n",
    "\n",
    "## 15. 체인 실행\n",
    "\n",
    "```python\n",
    "chain.invoke(\"How many ministries are mentioned\")\n",
    "```\n",
    "\n",
    "구성된 체인을 사용하여 질문에 대한 답변을 생성합니다.\n",
    "\n",
    "이 코드는 문서에서 정보를 추출하고 질문에 답변하는 복잡한 RAG 시스템을 구현합니다. 각 구성 요소가 서로 연계되어 효율적이고 정확한 정보 검색 및 답변 생성 프로세스를 만듭니다.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
