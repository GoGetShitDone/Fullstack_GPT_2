{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Victory Mansions is a building where Winston Smith resides. It is a run-down apartment complex with a faulty lift, gritty dust, and a hallway that smells of boiled cabbage and old rag mats. The flat where Winston lives is on the seventh floor, and the building is poorly maintained due to the ongoing economy drive in preparation for Hate Week. The building has a telescreen that cannot be completely shut off, and a poster with the caption \"BIG BROTHER IS WATCHING YOU\" hangs on the wall.')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#RunnablePassthrough(), \n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    ")\n",
    "\n",
    "cache_dir = LocalFileStore(\"./.cache/\")\n",
    "\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "loader = UnstructuredFileLoader(\"./rag_data/chapter_one.txt\")\n",
    "\n",
    "docs = loader.load_and_split(text_splitter=splitter)\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "cached_embeddings = CacheBackedEmbeddings.from_bytes_store(embeddings, cache_dir)\n",
    "\n",
    "vectorstore = FAISS.from_documents(docs, cached_embeddings)\n",
    "\n",
    "retriver = vectorstore.as_retriever()\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant. Answer questions using only the following context. If you don't know the answer just say you don't know, don't make it up:\\n\\n{context}\",\n",
    "        ),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        \"context\": retriver,\n",
    "        \"question\": RunnablePassthrough(),\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    ")\n",
    "\n",
    "chain.invoke(\"Describe Victory Mansions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='주인공은 윈스턴 스미스입니다.')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"주인공이 누구야?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='죄송합니다. 알려드릴 수 있는 정보가 없습니다.')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"왜?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='죄송합니다. 그 정보는 제가 알고 있는 내용에는 포함되어 있지 않습니다.')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"왜 주인공이 윈스턴 스미스야?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Three ministries are mentioned in the text: the Ministry of Love, the Ministry of Plenty, and the Ministry of Truth.')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Map Reduce LCEL(LangChain Expression Languege) Chain | \n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    ")\n",
    "\n",
    "cache_dir = LocalFileStore(\"./.cache/\")\n",
    "\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "loader = UnstructuredFileLoader(\"./rag_data/chapter_one.txt\")\n",
    "\n",
    "docs = loader.load_and_split(text_splitter=splitter)\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "cached_embeddings = CacheBackedEmbeddings.from_bytes_store(embeddings, cache_dir)\n",
    "\n",
    "vectorstore = FAISS.from_documents(docs, cached_embeddings)\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "\n",
    "map_doc_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"\n",
    "            Use the following portion of a long document to see if any of the text is relevant to answer the question. Return any relevant text verbatim. If there is no relevant text, return : ''\n",
    "            -------\n",
    "            {context}\n",
    "            \"\"\",\n",
    "        ),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "map_doc_chain = map_doc_prompt | llm\n",
    "\n",
    "\n",
    "def map_docs(inputs):\n",
    "    documents = inputs[\"documents\"]\n",
    "    question = inputs[\"question\"]\n",
    "    return \"\\n\\n\".join(\n",
    "        map_doc_chain.invoke(\n",
    "            {\"context\": doc.page_content, \"question\": question}\n",
    "        ).content\n",
    "        for doc in documents\n",
    "    )\n",
    "\n",
    "\n",
    "map_chain = {\n",
    "    \"documents\": retriever,\n",
    "    \"question\": RunnablePassthrough(),\n",
    "} | RunnableLambda(map_docs)\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"\n",
    "            Given the following extracted parts of a long document and a question, create a final answer. \n",
    "            If you don't know the answer, just say that you don't know. Don't try to make up an answer.\n",
    "            ------\n",
    "            {context}\n",
    "            \"\"\",\n",
    "        ),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = {\"context\": map_chain, \"question\": RunnablePassthrough()} | final_prompt | llm\n",
    "\n",
    "chain.invoke(\"How many ministries are mentioned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Winston goes to work at the Ministry of Truth.')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"where dose Winsthon go to work\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Winston is a smallish, frail figure with fair hair, a naturally sanguine face, and roughened skin from coarse soap, blunt razor blades, and the cold winter weather. He wears blue overalls, the uniform of the party. Winston sets his features into an expression of quiet optimism when facing the telescreen, drinks Victory Gin, smokes Victory Cigarettes, and is drawn to a man named O'Brien. He feels uneasiness and fear mixed with hostility around a particular girl and experiences a constricted diaphragm when seeing the face of Goldstein.\")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"Describe Winston\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG 시스템 코드 분석\n",
    "\n",
    "## 1. 필요한 라이브러리 및 클래스 임포트\n",
    "\n",
    "```python\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
    "```\n",
    "\n",
    "이 부분에서는 LangChain 라이브러리에서 필요한 모든 클래스와 함수를 임포트합니다. 각 임포트의 역할은 다음과 같습니다:\n",
    "\n",
    "- `ChatOpenAI`: OpenAI의 챗봇 모델을 사용하기 위한 클래스\n",
    "- `UnstructuredFileLoader`: 구조화되지 않은 파일(텍스트 파일 등)을 로드하기 위한 클래스\n",
    "- `CharacterTextSplitter`: 텍스트를 작은 청크로 분할하기 위한 클래스\n",
    "- `OpenAIEmbeddings`, `CacheBackedEmbeddings`: 텍스트를 벡터로 변환하기 위한 클래스들\n",
    "- `FAISS`: 벡터 데이터베이스를 생성하고 관리하기 위한 클래스\n",
    "- `LocalFileStore`: 로컬 파일 시스템에 데이터를 저장하기 위한 클래스\n",
    "- `ChatPromptTemplate`: 챗봇 프롬프트를 만들기 위한 클래스\n",
    "- `RunnablePassthrough`, `RunnableLambda`: 데이터 처리 파이프라인을 구성하기 위한 클래스들\n",
    "\n",
    "## 2. 언어 모델 초기화\n",
    "\n",
    "```python\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    ")\n",
    "```\n",
    "\n",
    "OpenAI의 챗봇 모델을 초기화합니다. `temperature=0.1`로 설정하여 모델의 출력을 더 결정적(덜 무작위적)으로 만듭니다.\n",
    "\n",
    "## 3. 캐시 디렉토리 설정\n",
    "\n",
    "```python\n",
    "cache_dir = LocalFileStore(\"./.cache/\")\n",
    "```\n",
    "\n",
    "임베딩을 캐시하기 위한 로컬 디렉토리를 설정합니다. 이는 반복적인 계산을 줄이고 성능을 향상시키는 데 도움이 됩니다.\n",
    "\n",
    "## 4. 텍스트 분할기 설정\n",
    "\n",
    "```python\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "```\n",
    "\n",
    "텍스트를 작은 청크로 나누기 위한 분할기를 설정합니다. 여기서는:\n",
    "- 줄바꿈(`\\n`)을 기준으로 텍스트를 나눕니다.\n",
    "- 각 청크의 크기는 600 토큰입니다.\n",
    "- 청크 간 100 토큰의 중복을 허용하여 문맥의 연속성을 유지합니다.\n",
    "\n",
    "## 5. 문서 로드 및 분할\n",
    "\n",
    "```python\n",
    "loader = UnstructuredFileLoader(\"./rag_data/chapter_one.txt\")\n",
    "docs = loader.load_and_split(text_splitter=splitter)\n",
    "```\n",
    "\n",
    "`UnstructuredFileLoader`를 사용하여 텍스트 파일을 로드하고, 앞서 정의한 `splitter`를 사용하여 이를 작은 청크로 나눕니다.\n",
    "\n",
    "## 6. 임베딩 설정\n",
    "\n",
    "```python\n",
    "embeddings = OpenAIEmbeddings()\n",
    "cached_embeddings = CacheBackedEmbeddings.from_bytes_store(embeddings, cache_dir)\n",
    "```\n",
    "\n",
    "OpenAI의 임베딩 모델을 초기화하고, 이를 캐시 기반 임베딩으로 래핑합니다. 이렇게 하면 이전에 계산된 임베딩을 재사용할 수 있어 효율성이 향상됩니다.\n",
    "\n",
    "## 7. 벡터 데이터베이스 생성\n",
    "\n",
    "```python\n",
    "vectorstore = FAISS.from_documents(docs, cached_embeddings)\n",
    "```\n",
    "\n",
    "FAISS를 사용하여 문서의 벡터 표현을 저장하는 데이터베이스를 생성합니다. 이는 효율적인 유사성 검색을 가능하게 합니다.\n",
    "\n",
    "## 8. 검색기(Retriever) 설정\n",
    "\n",
    "```python\n",
    "retriever = vectorstore.as_retriever()\n",
    "```\n",
    "\n",
    "벡터 데이터베이스를 검색기로 변환합니다. 이를 통해 질문과 관련된 문서를 효율적으로 검색할 수 있습니다.\n",
    "\n",
    "## 9. 문서 매핑 프롬프트 설정\n",
    "\n",
    "```python\n",
    "map_doc_prompt = ChatPromptTemplate.from_messages([...])\n",
    "```\n",
    "\n",
    "각 문서 청크를 처리하기 위한 프롬프트 템플릿을 정의합니다. 이 프롬프트는 시스템과 사용자 메시지로 구성되며, 문서의 관련 부분을 추출하는 데 사용됩니다.\n",
    "\n",
    "## 10. 문서 매핑 체인 설정\n",
    "\n",
    "```python\n",
    "map_doc_chain = map_doc_prompt | llm\n",
    "```\n",
    "\n",
    "프롬프트와 언어 모델을 연결하여 각 문서 청크를 처리하는 체인을 생성합니다.\n",
    "\n",
    "## 11. 문서 매핑 함수 정의\n",
    "\n",
    "```python\n",
    "def map_docs(inputs):\n",
    "    ...\n",
    "```\n",
    "\n",
    "이 함수는 검색된 모든 문서에 대해 `map_doc_chain`을 적용하고 결과를 결합합니다.\n",
    "\n",
    "## 12. 매핑 체인 설정\n",
    "\n",
    "```python\n",
    "map_chain = {\n",
    "    \"documents\": retriever,\n",
    "    \"question\": RunnablePassthrough(),\n",
    "} | RunnableLambda(map_docs)\n",
    "```\n",
    "\n",
    "검색기와 `map_docs` 함수를 결합하여 전체 매핑 프로세스를 정의합니다.\n",
    "\n",
    "## 13. 최종 프롬프트 설정\n",
    "\n",
    "```python\n",
    "final_prompt = ChatPromptTemplate.from_messages([...])\n",
    "```\n",
    "\n",
    "최종 답변을 생성하기 위한 프롬프트 템플릿을 정의합니다.\n",
    "\n",
    "## 14. 전체 체인 구성\n",
    "\n",
    "```python\n",
    "chain = {\"context\": map_chain, \"question\": RunnablePassthrough()} | final_prompt | llm\n",
    "```\n",
    "\n",
    "모든 구성 요소를 하나의 체인으로 결합합니다. 이 체인은:\n",
    "1. 질문을 받습니다.\n",
    "2. 관련 문서를 검색합니다.\n",
    "3. 검색된 문서에서 관련 정보를 추출합니다.\n",
    "4. 최종 답변을 생성합니다.\n",
    "\n",
    "## 15. 체인 실행\n",
    "\n",
    "```python\n",
    "chain.invoke(\"How many ministries are mentioned\")\n",
    "```\n",
    "\n",
    "구성된 체인을 사용하여 질문에 대한 답변을 생성합니다.\n",
    "\n",
    "이 코드는 문서에서 정보를 추출하고 질문에 답변하는 복잡한 RAG 시스템을 구현합니다. 각 구성 요소가 서로 연계되어 효율적이고 정확한 정보 검색 및 답변 생성 프로세스를 만듭니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain의 Runnable 컴포넌트 설명\n",
    "\n",
    "## 1. langchain.schema.runnable\n",
    "\n",
    "이는 LangChain에서 실행 가능한(runnable) 객체들의 기본 스키마를 정의하는 모듈입니다. 이 모듈은 다른 runnable 컴포넌트들의 기반이 되는 추상 클래스와 인터페이스를 제공합니다.\n",
    "\n",
    "주요 특징:\n",
    "- 실행 가능한 객체들의 기본 구조를 정의\n",
    "- 다른 runnable 컴포넌트들이 상속받아 사용\n",
    "\n",
    "## 2. RunnablePassthrough\n",
    "\n",
    "RunnablePassthrough는 입력을 그대로 출력으로 전달하는 간단한 runnable 컴포넌트입니다.\n",
    "\n",
    "주요 특징:\n",
    "- 입력을 수정하지 않고 그대로 다음 단계로 전달\n",
    "- 복잡한 체인에서 일부 데이터를 그대로 유지하고 싶을 때 유용\n",
    "- 파이프라인의 중간에서 데이터를 보존하거나 우회시킬 때 사용\n",
    "\n",
    "사용 예:\n",
    "```python\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "passthrough = RunnablePassthrough()\n",
    "result = passthrough.invoke(\"Hello, World!\")\n",
    "print(result)  # 출력: Hello, World!\n",
    "```\n",
    "\n",
    "## 3. RunnableLambda\n",
    "\n",
    "RunnableLambda는 사용자 정의 함수를 runnable 객체로 변환하는 컴포넌트입니다.\n",
    "\n",
    "주요 특징:\n",
    "- 일반 파이썬 함수를 LangChain의 runnable 인터페이스에 맞게 래핑\n",
    "- 복잡한 로직이나 데이터 변환을 체인에 쉽게 통합 가능\n",
    "- 유연한 데이터 처리와 변환을 가능하게 함\n",
    "\n",
    "사용 예:\n",
    "```python\n",
    "from langchain.schema.runnable import RunnableLambda\n",
    "\n",
    "def uppercase_function(text):\n",
    "    return text.upper()\n",
    "\n",
    "uppercase_runnable = RunnableLambda(uppercase_function)\n",
    "result = uppercase_runnable.invoke(\"hello, world!\")\n",
    "print(result)  # 출력: HELLO, WORLD!\n",
    "```\n",
    "\n",
    "## 실제 사용 시나리오\n",
    "\n",
    "이러한 컴포넌트들은 주로 복잡한 LangChain 파이프라인을 구축할 때 사용됩니다. 예를 들어:\n",
    "\n",
    "1. RunnablePassthrough를 사용하여 원본 질문을 파이프라인 전체에 걸쳐 유지\n",
    "2. RunnableLambda를 사용하여 검색된 문서를 처리하거나 포맷팅\n",
    "3. 이러한 컴포넌트들을 조합하여 질문-답변 시스템이나 문서 요약기 등을 구축\n",
    "\n",
    "이 컴포넌트들을 활용하면 유연하고 강력한 자연어 처리 파이프라인을 구축할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 파이썬의 Lambda 함수\n",
    "\n",
    "Lambda 함수는 파이썬에서 작은 익명 함수를 만드는 방법입니다. '익명'이라는 말은 이 함수가 이름을 갖지 않는다는 의미입니다.\n",
    "\n",
    "## 1. Lambda 함수의 특징\n",
    "\n",
    "- 한 줄로 정의되는 간단한 함수입니다.\n",
    "- 여러 개의 인자를 가질 수 있지만, 표현식은 하나만 가질 수 있습니다.\n",
    "- 주로 다른 함수의 인자로 사용되거나 즉시 실행되는 로직에 사용됩니다.\n",
    "\n",
    "## 2. Lambda 함수의 구조\n",
    "\n",
    "기본 구조는 다음과 같습니다:\n",
    "\n",
    "```python\n",
    "lambda arguments: expression\n",
    "```\n",
    "\n",
    "- `lambda`: 키워드로, lambda 함수를 정의한다는 것을 나타냅니다.\n",
    "- `arguments`: 함수의 인자들입니다. 여러 개일 수 있습니다.\n",
    "- `expression`: 함수의 본문으로, 단일 표현식만 가능합니다.\n",
    "\n",
    "## 3. Lambda 함수 예시\n",
    "\n",
    "### 예시 1: 단순 계산\n",
    "```python\n",
    "# 제곱을 계산하는 lambda 함수\n",
    "square = lambda x: x**2\n",
    "print(square(5))  # 출력: 25\n",
    "```\n",
    "\n",
    "### 예시 2: 정렬에 사용\n",
    "```python\n",
    "# 리스트를 두 번째 요소를 기준으로 정렬\n",
    "pairs = [(1, 'one'), (2, 'two'), (3, 'three'), (4, 'four')]\n",
    "pairs.sort(key=lambda pair: pair[1])\n",
    "print(pairs)  # 출력: [(4, 'four'), (1, 'one'), (3, 'three'), (2, 'two')]\n",
    "```\n",
    "\n",
    "### 예시 3: 함수 내에서 사용\n",
    "```python\n",
    "# map 함수와 함께 사용\n",
    "numbers = [1, 2, 3, 4]\n",
    "squared = list(map(lambda x: x**2, numbers))\n",
    "print(squared)  # 출력: [1, 4, 9, 16]\n",
    "```\n",
    "\n",
    "## 4. Lambda vs 일반 함수\n",
    "\n",
    "Lambda 함수는 간단한 연산에 유용하지만, 복잡한 로직에는 일반 함수가 더 적합합니다.\n",
    "\n",
    "```python\n",
    "# Lambda 함수\n",
    "multiply = lambda x, y: x * y\n",
    "\n",
    "# 동일한 기능의 일반 함수\n",
    "def multiply(x, y):\n",
    "    return x * y\n",
    "```\n",
    "\n",
    "Lambda 함수는 코드를 간결하게 만들 수 있지만, 과도한 사용은 가독성을 떨어뜨릴 수 있으므로 적절히 사용하는 것이 중요합니다.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
