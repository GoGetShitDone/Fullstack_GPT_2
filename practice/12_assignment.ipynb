{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "memory = ConversationBufferMemory(return_messages=True, return_message=True,)\n",
    "\n",
    "cache_dir = LocalFileStore(\"./.cache/\")\n",
    "\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "\n",
    "loader = UnstructuredFileLoader(\"./rag_data/chapter_3.txt\")\n",
    "docs = loader.load_and_split(text_splitter=splitter)\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "cached_embeddings = CacheBackedEmbeddings.from_bytes_store(embeddings, cache_dir)\n",
    "\n",
    "vectorstore = FAISS.from_documents(docs, cached_embeddings)\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "Use the following pieces of context to answer the question at the end. \n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer. {context}\n",
    "\n",
    "Question: {question}\n",
    "Answer: \n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template = prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "chain = RetrievalQA.from_chain_type(\n",
    "\tllm=llm,\n",
    "\tchain_type=\"stuff\", \n",
    "\tretriever=vectorstore.as_retriever(),\n",
    "\tmemory=memory,\n",
    "\tchain_type_kwargs={\"prompt\":prompt},\n",
    ")\n",
    "\n",
    "def invoke_chain(question):\n",
    "\tresult = chain.invoke({\"query\":question})\n",
    "\tprint(f\"Question: {question}\")\n",
    "\tprint(f\"Answer: {result['result']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Aaronson 은 유죄인가요?\n",
      "Answer: I don't know.\n"
     ]
    }
   ],
   "source": [
    "invoke_chain(\"Aaronson 은 유죄인가요?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: 그가 테이블에 어떤 메시지를 썼나요?\n",
      "Answer: 그가 적은 메시지는 \"FREEDOM IS SLAVERY\"과 \"TWO AND TWO MAKE FIVE\"이었습니다.\n"
     ]
    }
   ],
   "source": [
    "invoke_chain(\"그가 테이블에 어떤 메시지를 썼나요?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Julia 는 누구인가요?\n",
      "Answer: Julia is a character in the novel who is involved in a romantic relationship with Winston, the main character.\n"
     ]
    }
   ],
   "source": [
    "invoke_chain(\"Julia 는 누구인가요?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
